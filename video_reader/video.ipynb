{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "from video import Video\n",
    "path = '../videos/SOX5yA1l24A.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available streams: (id, stream_type, stream)\n",
      "\t 0, video, <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f336c8ae750> \n",
      "\t 1, audio, <av.AudioStream #1 aac at 48000Hz, mono, fltp at 0x7f336c8ae6e0> \n",
      "../videos/SOX5yA1l24A.mp4 \n",
      " \t default stream:  <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f336c8ae750> \n",
      " \t keyframes only:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjuncek/work/video_reader_benchmark/video_reader/video.py:37: UserWarning: Stream given as a descriptive string, will return the first stream of that type if it exists\n",
      "  warnings.warn(\"Stream given as a descriptive string, will return the first stream of that type if it exists\")\n"
     ]
    }
   ],
   "source": [
    "test_object = Video(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available streams: (id, stream_type, stream)\n",
      "\t 0, video, <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f66b307ea60> \n",
      "\t 1, audio, <av.AudioStream #1 aac at 48000Hz, mono, fltp at 0x7f66b307e980> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'video',\n",
       "  'stream': <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f66b307ea60>},\n",
       " {'type': 'audio',\n",
       "  'stream': <av.AudioStream #1 aac at 48000Hz, mono, fltp at 0x7f66b307e980>}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_object.list_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing rudimentary read video function\n",
    "\n",
    "### That supports most of the functionality from the current TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "torchvision.set_video_backend(\"pyav\")\n",
    "def get_tv(path):\n",
    "    vframes, aframes, _ = torchvision.io.read_video(path, 8, 10, pts_unit=\"sec\")\n",
    "    return vframes.size(), aframes.size()\n",
    "\n",
    "sa, sb = get_tv(path)\n",
    "sizes = f\"Expected sizes: {sa}, {sb}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as t\n",
    "\n",
    "def read_video(vid, start=0, end=None, height=-1, width=-1, read_video=True, read_audio=True):\n",
    "    if not isinstance(vid, Video):\n",
    "        vid = Video(path)\n",
    "    \n",
    "    # safety checks - basic stuff\n",
    "    if end is None:\n",
    "        end = float(\"inf\")\n",
    "    if end < start:\n",
    "        raise ValueError(\n",
    "            \"end_pts should be larger than start_pts, got \"\n",
    "            \"start_pts={} and end_pts={}\".format(start_pts, end_pts)\n",
    "        )\n",
    "    \n",
    "    # safety checks, streams\n",
    "    stream_types = [x['type'] for x in vid.available_streams]\n",
    "    if read_video:\n",
    "        assert \"video\" in stream_types\n",
    "    if read_audio:\n",
    "        assert \"audio\" in stream_types\n",
    "    \n",
    "    # get video_transform to apply per frame\n",
    "    # should save on memory\n",
    "    transforms = [t.ToTensor()]\n",
    "    if width > 0 and height>0:\n",
    "            transforms.insert(0, t.Resize((height, width), interpolation=2))\n",
    "            transforms.insert(0, t.ToPILImage())     \n",
    "    frame_transform = t.Compose(transforms)\n",
    "    \n",
    "    current_pts = start\n",
    "    if read_video:\n",
    "        video_frames = [] # video frame buffer \n",
    "    if read_audio:\n",
    "        audio_frames = [] # audio frame buffer\n",
    "    \n",
    "    # this should get us close to the actual starting point we want\n",
    "    if read_video:\n",
    "        vid.seek(start, stream=\"video\")\n",
    "    if read_audio:\n",
    "        vid.seek(start, stream=\"audio\")\n",
    "    \n",
    "    while current_pts <= end:\n",
    "        if read_video:\n",
    "            frame, current_pts, stream_t = vid.next(\"video\")\n",
    "            assert stream_t == \"video\"\n",
    "            if current_pts >= start and current_pts <= end:\n",
    "                video_frames.append(frame_transform(frame))\n",
    "        if read_audio:\n",
    "            frame, current_pts, stream_t = vid.next(\"audio\")\n",
    "            assert stream_t == \"audio\"\n",
    "            if current_pts >= start and current_pts <= end:\n",
    "                audio_frames.append(torch.tensor(frame))\n",
    "    \n",
    "    output = {'video': torch.stack(video_frames, 0) if read_video else torch.empty(0),\n",
    "              'audio': torch.stack(audio_frames, 0) if read_audio else torch.empty(0)}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available streams: (id, stream_type, stream)\n",
      "\t 0, video, <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f65057df210> \n",
      "\t 1, audio, <av.AudioStream #1 aac at 48000Hz, mono, fltp at 0x7f65057df3d0> \n",
      "../videos/SOX5yA1l24A.mp4 \n",
      " \t default stream:  <av.VideoStream #0 h264, yuv420p 340x256 at 0x7f65057df210> \n",
      " \t keyframes only:  False\n",
      "Expected sizes: torch.Size([61, 256, 340, 3]), torch.Size([1, 0])\n",
      "torch.Size([60, 3, 256, 340]) torch.Size([60, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "from video import Video\n",
    "path = '../videos/SOX5yA1l24A.mp4'\n",
    "test_object = Video(path)\n",
    "\n",
    "test = read_video(test_object, 0, 2)\n",
    "\n",
    "print(sizes)\n",
    "print(test['video'].size(), test['audio'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 384, 340]) torch.Size([60, 1, 1024])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
